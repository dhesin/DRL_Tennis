{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from ddpg_agent import Agent\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are (2, 24) agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size_env = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size_env)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size_env = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape, state_size_env))\n",
    "print('The state for the first agent looks like:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\tAverage Score: 0.007\tScore: 0.100\tBalls Over: 11        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 400\tAverage Score: 0.031\tScore: 0.000\tBalls Over: 57        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 600\tAverage Score: 0.035\tScore: 0.000\tBalls Over: 145        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 800\tAverage Score: 0.027\tScore: 0.000\tBalls Over: 215        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 1000\tAverage Score: 0.052\tScore: 0.100\tBalls Over: 317        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 1200\tAverage Score: 0.071\tScore: 0.100\tBalls Over: 486        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 1400\tAverage Score: 0.083\tScore: 0.100\tBalls Over: 730        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 1600\tAverage Score: 0.162\tScore: 0.200\tBalls Over: 1153        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 1800\tAverage Score: 0.324\tScore: 0.100\tBalls Over: 2089        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 2000\tAverage Score: 0.524\tScore: 0.200\tBalls Over: 4310        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 2200\tAverage Score: 0.321\tScore: 0.400\tBalls Over: 5537        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 2400\tAverage Score: 0.360\tScore: 0.000\tBalls Over: 7190        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 2600\tAverage Score: 0.565\tScore: 0.300\tBalls Over: 8848        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 2800\tAverage Score: 0.767\tScore: 0.100\tBalls Over: 11407        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 3000\tAverage Score: 0.552\tScore: 0.190\tBalls Over: 14055        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 3200\tAverage Score: 0.481\tScore: 0.900\tBalls Over: 15906        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 3400\tAverage Score: 0.490\tScore: 0.300\tBalls Over: 17701        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 3600\tAverage Score: 0.483\tScore: 0.300\tBalls Over: 19382        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 3800\tAverage Score: 0.380\tScore: 0.100\tBalls Over: 21050        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 4000\tAverage Score: 0.376\tScore: 0.700\tBalls Over: 22596        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 4200\tAverage Score: 0.511\tScore: 1.300\tBalls Over: 24470        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 4400\tAverage Score: 0.398\tScore: 0.100\tBalls Over: 25995        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 4600\tAverage Score: 0.554\tScore: 0.700\tBalls Over: 27855        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 4800\tAverage Score: 0.414\tScore: 0.700\tBalls Over: 29843        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 5000\tAverage Score: 0.514\tScore: 0.300\tBalls Over: 31788        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 5200\tAverage Score: 0.539\tScore: 0.400\tBalls Over: 34037        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 5400\tAverage Score: 0.494\tScore: 0.300\tBalls Over: 36145        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 5600\tAverage Score: 0.433\tScore: 0.300\tBalls Over: 37412        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 5800\tAverage Score: 0.245\tScore: 0.100\tBalls Over: 38685        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 6000\tAverage Score: 0.218\tScore: 0.100\tBalls Over: 39489        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 6200\tAverage Score: 0.417\tScore: 0.100\tBalls Over: 40764        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 6400\tAverage Score: 0.286\tScore: 0.100\tBalls Over: 41681        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 6600\tAverage Score: 0.524\tScore: 0.000\tBalls Over: 43345        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 6800\tAverage Score: 0.491\tScore: 0.300\tBalls Over: 45047        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 7000\tAverage Score: 0.609\tScore: 0.100\tBalls Over: 47225        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 7200\tAverage Score: 0.371\tScore: 0.100\tBalls Over: 48728        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 7400\tAverage Score: 0.271\tScore: 0.300\tBalls Over: 49922        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 7600\tAverage Score: 0.444\tScore: 1.300\tBalls Over: 51483        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 7800\tAverage Score: 0.557\tScore: 1.400\tBalls Over: 53637        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 8000\tAverage Score: 0.429\tScore: 0.190\tBalls Over: 55423        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 8200\tAverage Score: 0.332\tScore: 0.200\tBalls Over: 56628        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 8400\tAverage Score: 0.268\tScore: 0.000\tBalls Over: 58402        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 8600\tAverage Score: 0.356\tScore: 0.000\tBalls Over: 59512        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 8800\tAverage Score: 0.298\tScore: 0.200\tBalls Over: 60405        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 9000\tAverage Score: 0.307\tScore: 0.090\tBalls Over: 61541        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 9200\tAverage Score: 0.275\tScore: 0.200\tBalls Over: 62593        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 9400\tAverage Score: 0.226\tScore: 0.100\tBalls Over: 63574        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 9600\tAverage Score: 0.183\tScore: 0.100\tBalls Over: 64272        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 9800\tAverage Score: 0.196\tScore: 0.100\tBalls Over: 65120        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 10000\tAverage Score: 0.185\tScore: 0.100\tBalls Over: 65757        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 10200\tAverage Score: 0.171\tScore: 0.100\tBalls Over: 66300        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 10400\tAverage Score: 0.204\tScore: 0.200\tBalls Over: 66922        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 10600\tAverage Score: 0.218\tScore: 0.100\tBalls Over: 67625        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 10800\tAverage Score: 0.174\tScore: 0.100\tBalls Over: 68245        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 11000\tAverage Score: 0.303\tScore: 0.200\tBalls Over: 69302        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 11200\tAverage Score: 0.503\tScore: 0.500\tBalls Over: 71020        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 11400\tAverage Score: 0.305\tScore: 0.200\tBalls Over: 72272        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 11600\tAverage Score: 0.257\tScore: 0.200\tBalls Over: 73446        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 11800\tAverage Score: 0.237\tScore: 0.390\tBalls Over: 74330        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 12000\tAverage Score: 0.352\tScore: 0.500\tBalls Over: 75421        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 12200\tAverage Score: 0.242\tScore: 0.100\tBalls Over: 76109        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 12400\tAverage Score: 0.455\tScore: 0.600\tBalls Over: 77668        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 12600\tAverage Score: 0.356\tScore: 0.100\tBalls Over: 79055        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 12800\tAverage Score: 0.307\tScore: 0.500\tBalls Over: 80114        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 13000\tAverage Score: 0.414\tScore: 0.100\tBalls Over: 81473        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 13200\tAverage Score: 0.255\tScore: 0.100\tBalls Over: 82426        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 13400\tAverage Score: 0.308\tScore: 0.300\tBalls Over: 83420        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 13600\tAverage Score: 0.342\tScore: 0.300\tBalls Over: 84451        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 13800\tAverage Score: 0.413\tScore: 1.100\tBalls Over: 85821        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 14000\tAverage Score: 0.646\tScore: 2.200\tBalls Over: 87958        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 14200\tAverage Score: 0.479\tScore: 0.100\tBalls Over: 89841        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 14400\tAverage Score: 0.299\tScore: 0.400\tBalls Over: 91146        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 14600\tAverage Score: 0.730\tScore: 0.800\tBalls Over: 93482        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 14800\tAverage Score: 0.441\tScore: 0.300\tBalls Over: 94869        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 15000\tAverage Score: 0.245\tScore: 0.190\tBalls Over: 95778        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 15200\tAverage Score: 0.524\tScore: 0.200\tBalls Over: 97553        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 15400\tAverage Score: 0.199\tScore: 0.100\tBalls Over: 98610        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 15600\tAverage Score: 0.099\tScore: 0.100\tBalls Over: 98970        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 15800\tAverage Score: 0.184\tScore: 0.300\tBalls Over: 99455        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 16000\tAverage Score: 0.133\tScore: 0.000\tBalls Over: 99845        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 16200\tAverage Score: 0.129\tScore: 0.100\tBalls Over: 100228        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 16400\tAverage Score: 0.120\tScore: 0.100\tBalls Over: 100578        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 16600\tAverage Score: 0.116\tScore: 0.100\tBalls Over: 100903        [1.0101010101010103e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16800\tAverage Score: 0.143\tScore: 0.200\tBalls Over: 101366        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 17000\tAverage Score: 0.121\tScore: 0.100\tBalls Over: 101760        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 17200\tAverage Score: 0.173\tScore: 0.100\tBalls Over: 102293        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 17400\tAverage Score: 0.777\tScore: 0.100\tBalls Over: 104329        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 17600\tAverage Score: 0.674\tScore: 2.600\tBalls Over: 107149        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 17800\tAverage Score: 0.759\tScore: 0.300\tBalls Over: 109614        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 18000\tAverage Score: 0.458\tScore: 1.300\tBalls Over: 111430        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 18200\tAverage Score: 0.328\tScore: 0.000\tBalls Over: 112848        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 18400\tAverage Score: 0.242\tScore: 0.200\tBalls Over: 113771        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 18600\tAverage Score: 0.247\tScore: 0.200\tBalls Over: 114685        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 18800\tAverage Score: 0.250\tScore: 0.300\tBalls Over: 115605        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 19000\tAverage Score: 0.187\tScore: 0.100\tBalls Over: 116178        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 19200\tAverage Score: 0.201\tScore: 0.200\tBalls Over: 116900        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 19400\tAverage Score: 0.126\tScore: 0.000\tBalls Over: 117367        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 19600\tAverage Score: 0.139\tScore: 0.000\tBalls Over: 117835        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 19800\tAverage Score: 0.188\tScore: 0.100\tBalls Over: 118393        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 20000\tAverage Score: 0.215\tScore: 0.200\tBalls Over: 119050        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 20200\tAverage Score: 0.250\tScore: 1.200\tBalls Over: 119924        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 20400\tAverage Score: 0.185\tScore: 0.200\tBalls Over: 120691        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 20600\tAverage Score: 0.362\tScore: 0.600\tBalls Over: 121830        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 20800\tAverage Score: 0.212\tScore: 0.200\tBalls Over: 122550        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 21000\tAverage Score: 0.288\tScore: 0.000\tBalls Over: 123439        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 21200\tAverage Score: 0.179\tScore: 0.600\tBalls Over: 124033        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 21400\tAverage Score: 0.231\tScore: 0.500\tBalls Over: 124782        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 21600\tAverage Score: 0.490\tScore: 0.200\tBalls Over: 126569        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 21800\tAverage Score: 0.441\tScore: 0.100\tBalls Over: 128273        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 22000\tAverage Score: 0.391\tScore: 0.700\tBalls Over: 129812        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 22200\tAverage Score: 0.283\tScore: 0.100\tBalls Over: 130892        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 22400\tAverage Score: 0.341\tScore: 0.700\tBalls Over: 132051        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 22600\tAverage Score: 0.245\tScore: 0.400\tBalls Over: 133087        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 22800\tAverage Score: 0.312\tScore: 0.100\tBalls Over: 134374        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 23000\tAverage Score: 0.831\tScore: 0.500\tBalls Over: 136603        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 23200\tAverage Score: 0.473\tScore: 0.100\tBalls Over: 139098        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 23400\tAverage Score: 0.243\tScore: 0.100\tBalls Over: 140114        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 23600\tAverage Score: 0.386\tScore: 0.400\tBalls Over: 141282        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 23800\tAverage Score: 0.303\tScore: 0.200\tBalls Over: 142532        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 24000\tAverage Score: 0.569\tScore: 0.100\tBalls Over: 144342        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 24200\tAverage Score: 0.913\tScore: 0.400\tBalls Over: 147140        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 24400\tAverage Score: 0.660\tScore: 0.400\tBalls Over: 149496        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 24600\tAverage Score: 0.368\tScore: 0.100\tBalls Over: 150741        [1.0101010101010103e-05]\n",
      "\n",
      "Episode 24790\tAverage Score: 0.454\tScore: 0.100\tBalls Over: 152140        "
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size_env, action_size=action_size_env, num_agents=2, random_seed=0)\n",
    "#agent = Agent(state_size=state_size_env, action_size=action_size_env, actor_chkpt_file=\"checkpoint_actor.pth\", critic_chkpt_file=\"checkpoint_critic.pth\", random_seed=0)\n",
    "#agent_2 = Agent(state_size=state_size_env, action_size=action_size_env, random_seed=0)\n",
    "\n",
    "def ddpg(n_episodes=25000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_all = []\n",
    "    balls_over = 0\n",
    "    noise_scale = 1.0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        agent.reset()\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                         # initialize the score (for each agent)    \n",
    "        actions = np.zeros((num_agents, action_size_env))\n",
    "        #agent.reset()\n",
    "        t = 0\n",
    "        while True:\n",
    "            \n",
    "            actions = agent.act(states, True, noise_scale)\n",
    "\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            done = env_info.local_done                         # see if episode finished\n",
    "            scores += rewards                                  # update the score (for each agent\n",
    "        \n",
    "            if rewards[0] >= 0.1 or rewards[1] >= 0.1:\n",
    "                balls_over += 1\n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, done)\n",
    "\n",
    "            states = next_states\n",
    "            \n",
    "            if np.any(done):                                  # exit loop if episode finished\n",
    "                break \n",
    "                \n",
    "        noise_scale = noise_scale*0.99\n",
    "        scores_avg = np.amax(scores)\n",
    "        scores_deque.append(scores_avg)\n",
    "        scores_all.append(scores_avg)\n",
    "        #print(scores_agents)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:2.3f}\\tScore: {:2.3f}\\tBalls Over: {}        '.format(i_episode, np.mean(scores_deque), scores_avg, balls_over), end=\"\")\n",
    "        if i_episode % 200 == 0:\n",
    "            print(agent.actor_scheduler.get_lr())\n",
    "            print('\\nEpisode {}\\tAverage Score: {:2.3f}\\tScore: {:2.3f}\\tBalls Over: {}       '.format(i_episode, np.mean(scores_deque), scores_avg, balls_over), end=\"\")\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_{}.pth'.format(i_episode))\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_{}.pth'.format(i_episode))   \n",
    "    return scores_all\n",
    "\n",
    "#scores = ddpg(n_episodes=10, max_t=130)\n",
    "\n",
    "\n",
    "scores = ddpg()\n",
    "print('scores len  {}'.format(len(scores)))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "fig.savefig('AverageScore.png')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
